{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "import random"
      ],
      "metadata": {
        "id": "OIK4N2YN0PAi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_conllu(file_path):\n",
        "    # Return list of sentences, each sentence is list of (word, upos)\n",
        "    sentences = []\n",
        "    cur = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line == \"\" or line.startswith(\"#\"):\n",
        "                if cur:\n",
        "                    sentences.append(cur)\n",
        "                    cur = []\n",
        "                continue\n",
        "            parts = line.split('\\t')\n",
        "            if len(parts) < 5: continue\n",
        "            # column 2 is FORM (index 1), column 4 is UPOS (index 3)\n",
        "            form = parts[1]\n",
        "            upos = parts[3]\n",
        "            # skip multiword lines or comments handled above; also ignore if id contains '-'\n",
        "            if '-' in parts[0] or '.' in parts[0]:\n",
        "                continue\n",
        "            cur.append((form, upos)) # last ln\n",
        "        if cur:\n",
        "            sentences.append(cur)\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "UfimZQcW0bwf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocabs(train_sentences, min_freq=1):\n",
        "    word_counter = Counter()\n",
        "    tag_set = set()\n",
        "    for sent in train_sentences:\n",
        "        for w, t in sent:\n",
        "            word_counter[w] += 1\n",
        "            tag_set.add(t)\n",
        "    # special tokens\n",
        "    word_to_ix = {'<PAD>':0, '<UNK>':1}\n",
        "    for w, cnt in word_counter.items():\n",
        "        if cnt >= min_freq and w not in word_to_ix:\n",
        "            word_to_ix[w] = len(word_to_ix)\n",
        "    tag_to_ix = {'<PAD>':0}\n",
        "    for t in sorted(tag_set):\n",
        "        tag_to_ix[t] = len(tag_to_ix)\n",
        "    return word_to_ix, tag_to_ix"
      ],
      "metadata": {
        "id": "C7rx81890eYH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Dataset ----------\n",
        "class POSDataset(Dataset):\n",
        "    def __init__(self, sentences, word_to_ix, tag_to_ix):\n",
        "        self.sentences = sentences\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent = self.sentences[idx]\n",
        "        words = [w for w,t in sent]\n",
        "        tags = [t for w,t in sent]\n",
        "        word_indices = [ self.word_to_ix.get(w, self.word_to_ix['<UNK>']) for w in words ]\n",
        "        tag_indices = [ self.tag_to_ix[t] for t in tags ]\n",
        "        return torch.tensor(word_indices, dtype=torch.long), torch.tensor(tag_indices, dtype=torch.long)"
      ],
      "metadata": {
        "id": "gzGiSkG80n8o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# collate fn for variable-length sequences\n",
        "def collate_fn(batch):\n",
        "    word_seqs = [item[0] for item in batch]\n",
        "    tag_seqs = [item[1] for item in batch]\n",
        "    lengths = [len(s) for s in word_seqs]\n",
        "    words_padded = pad_sequence(word_seqs, batch_first=True, padding_value=0)  # pad word idx 0\n",
        "    tags_padded = pad_sequence(tag_seqs, batch_first=True, padding_value=0)    # pad tag idx 0 (ignore_index)\n",
        "    return words_padded, tags_padded, torch.tensor(lengths, dtype=torch.long)"
      ],
      "metadata": {
        "id": "tuY91dEW0uEG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Model ----------\n",
        "class SimpleRNNForTokenClassification(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_tags, padding_idx=0, num_layers=1, bidirectional=False):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
        "        self.rnn = nn.RNN(input_size=embed_dim,\n",
        "                          hidden_size=hidden_dim,\n",
        "                          num_layers=num_layers,\n",
        "                          batch_first=True,\n",
        "                          bidirectional=bidirectional)\n",
        "        rnn_out_dim = hidden_dim * (2 if bidirectional else 1)\n",
        "        self.fc = nn.Linear(rnn_out_dim, num_tags)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len)\n",
        "        emb = self.embedding(x)                       # (batch, seq_len, embed_dim)\n",
        "        rnn_out, _ = self.rnn(emb)                    # (batch, seq_len, hidden)\n",
        "        logits = self.fc(rnn_out)                     # (batch, seq_len, num_tags)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "JNn6Pcmn1C_h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Training and Evaluation ----------\n",
        "def compute_accuracy(preds, labels, pad_idx=0):\n",
        "    # preds, labels: flattened 1D tensors\n",
        "    mask = labels != pad_idx\n",
        "    if mask.sum().item() == 0:\n",
        "        return 0.0\n",
        "    correct = (preds[mask] == labels[mask]).sum().item()\n",
        "    total = mask.sum().item()\n",
        "    return correct / total\n",
        "\n",
        "def evaluate(model, dataloader, device, tag_pad_idx=0):\n",
        "    model.eval()\n",
        "    all_acc = []\n",
        "    total_loss = 0.0\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tag_pad_idx)\n",
        "    with torch.no_grad():\n",
        "        for words, tags, lengths in dataloader:\n",
        "            words = words.to(device)\n",
        "            tags = tags.to(device)\n",
        "            logits = model(words)  # (b, seq, num_tags)\n",
        "            b, seq, num_tags = logits.size()\n",
        "            logits_flat = logits.view(-1, num_tags)\n",
        "            tags_flat = tags.view(-1)\n",
        "            loss = criterion(logits_flat, tags_flat)\n",
        "            total_loss += loss.item() * words.size(0)\n",
        "            preds = torch.argmax(logits, dim=-1).view(-1)\n",
        "            acc = compute_accuracy(preds, tags_flat, pad_idx=tag_pad_idx)\n",
        "            all_acc.append((acc, words.size(0)))\n",
        "    if len(all_acc) == 0:\n",
        "        return 0.0, 0.0\n",
        "    # weighted average accuracy\n",
        "    weighted_acc = sum(a * n for a,n in all_acc) / sum(n for a,n in all_acc)\n",
        "    avg_loss = total_loss / sum(batch[0].size(0) for batch in dataloader)\n",
        "    return weighted_acc, avg_loss\n",
        "\n",
        "def train(model, train_loader, dev_loader, device, tag_pad_idx=0, epochs=5, lr=1e-3):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tag_pad_idx)\n",
        "    model.to(device)\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        total_batches = 0\n",
        "        for words, tags, lengths in train_loader:\n",
        "            words = words.to(device)\n",
        "            tags = tags.to(device)\n",
        "            logits = model(words)  # (b, seq, num_tags)\n",
        "            b, seq, num_tags = logits.size()\n",
        "            logits_flat = logits.view(-1, num_tags)\n",
        "            tags_flat = tags.view(-1)\n",
        "            loss = criterion(logits_flat, tags_flat)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            total_batches += 1\n",
        "        avg_train_loss = running_loss / max(1, total_batches)\n",
        "        train_acc, _ = evaluate(model, train_loader, device, tag_pad_idx=tag_pad_idx)\n",
        "        dev_acc, dev_loss = evaluate(model, dev_loader, device, tag_pad_idx=tag_pad_idx)\n",
        "        print(f\"Epoch {epoch}/{epochs} — train_loss: {avg_train_loss:.4f} | train_acc: {train_acc:.4f} | dev_loss: {dev_loss:.4f} | dev_acc: {dev_acc:.4f}\")"
      ],
      "metadata": {
        "id": "1bVHmrjP4afP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- prediction helper ----------\n",
        "def predict_sentence(model, sentence, word_to_ix, ix_to_tag, device):\n",
        "    model.eval()\n",
        "    tokens = sentence.strip().split()\n",
        "    indices = [ word_to_ix.get(w, word_to_ix['<UNK>']) for w in tokens ]\n",
        "    x = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)  # (1, seq, num_tags)\n",
        "        preds = torch.argmax(logits, dim=-1).squeeze(0).cpu().tolist()\n",
        "    tagged = [(tokens[i], ix_to_tag[preds[i]]) for i in range(len(tokens))]\n",
        "    return tagged"
      ],
      "metadata": {
        "id": "x5GJ9Hpw4wqo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wRqExlHbz_pC",
        "outputId": "cbf36f5b-7021-4068-94f6-97cb76a9ec19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 12544 train sentences, 2001 dev sentences\n",
            "Vocab sizes: words = 19675 , tags = 18\n",
            "Epoch 1/6 — train_loss: 1.6433 | train_acc: 0.6513 | dev_loss: 1.2147 | dev_acc: 0.6279\n",
            "Epoch 2/6 — train_loss: 0.9851 | train_acc: 0.7318 | dev_loss: 0.9616 | dev_acc: 0.7009\n",
            "Epoch 3/6 — train_loss: 0.7871 | train_acc: 0.7765 | dev_loss: 0.8441 | dev_acc: 0.7449\n",
            "Epoch 4/6 — train_loss: 0.6689 | train_acc: 0.8068 | dev_loss: 0.7765 | dev_acc: 0.7684\n",
            "Epoch 5/6 — train_loss: 0.5836 | train_acc: 0.8298 | dev_loss: 0.7324 | dev_acc: 0.7846\n",
            "Epoch 6/6 — train_loss: 0.5168 | train_acc: 0.8486 | dev_loss: 0.7073 | dev_acc: 0.7979\n",
            "Final dev accuracy: 0.7979\n",
            "Input: I love NLP\n",
            "Prediction: [('I', 'PRON'), ('love', 'VERB'), ('NLP', 'VERB')]\n",
            "----------------------------------------\n",
            "Input: Im about to blow\n",
            "Prediction: [('Im', 'VERB'), ('about', 'ADP'), ('to', 'ADP'), ('blow', 'VERB')]\n",
            "----------------------------------------\n",
            "Input: Im a Lion Piza Chicken\n",
            "Prediction: [('Im', 'VERB'), ('a', 'DET'), ('Lion', 'NOUN'), ('Piza', 'NOUN'), ('Chicken', 'NOUN')]\n",
            "----------------------------------------\n",
            "Input: You diggn in me\n",
            "Prediction: [('You', 'PRON'), ('diggn', 'VERB'), ('in', 'ADP'), ('me', 'PRON')]\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ---------- main runnable ----------\n",
        "def main():\n",
        "    # paths (adjust if needed)\n",
        "    train_path = \"/content/en_ewt-ud-train.conllu\"\n",
        "    dev_path   = \"/content/en_ewt-ud-dev.conllu\"\n",
        "    assert os.path.exists(train_path), f\"{train_path} not found\"\n",
        "    assert os.path.exists(dev_path), f\"{dev_path} not found\"\n",
        "\n",
        "    # load\n",
        "    train_sents = load_conllu(train_path)\n",
        "    dev_sents = load_conllu(dev_path)\n",
        "    print(f\"Loaded {len(train_sents)} train sentences, {len(dev_sents)} dev sentences\")\n",
        "\n",
        "    # build vocabs\n",
        "    word_to_ix, tag_to_ix = build_vocabs(train_sents, min_freq=1)\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    print(\"Vocab sizes: words =\", len(word_to_ix), \", tags =\", len(tag_to_ix))\n",
        "\n",
        "    # datasets and loaders\n",
        "    train_dataset = POSDataset(train_sents, word_to_ix, tag_to_ix)\n",
        "    dev_dataset = POSDataset(dev_sents, word_to_ix, tag_to_ix)\n",
        "    BATCH_SIZE = 64\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "    dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # model hyperparams\n",
        "    vocab_size = len(word_to_ix)\n",
        "    num_tags = len(tag_to_ix)\n",
        "    embed_dim = 100\n",
        "    hidden_dim = 128\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = SimpleRNNForTokenClassification(vocab_size=vocab_size, embed_dim=embed_dim, hidden_dim=hidden_dim, num_tags=num_tags, padding_idx=0, bidirectional=False)\n",
        "\n",
        "    # train\n",
        "    train(model, train_loader, dev_loader, device, tag_pad_idx=tag_to_ix['<PAD>'], epochs=6, lr=5e-4)\n",
        "\n",
        "    # final evaluation\n",
        "    dev_acc, _ = evaluate(model, dev_loader, device, tag_pad_idx=tag_to_ix['<PAD>'])\n",
        "    print(f\"Final dev accuracy: {dev_acc:.4f}\")\n",
        "\n",
        "    examples = [\n",
        "        \"I love NLP\",\n",
        "        \"Im about to blow\",\n",
        "        \"Im a Lion Piza Chicken\",\n",
        "        \"You diggn in me\"\n",
        "    ]\n",
        "\n",
        "    for example in examples:\n",
        "        tagged = predict_sentence(model, example, word_to_ix, ix_to_tag, device)\n",
        "        print(f\"Input: {example}\")\n",
        "        print(\"Prediction:\", tagged)\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    random.seed(42)\n",
        "    main()\n"
      ]
    }
  ]
}